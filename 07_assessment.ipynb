{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLI_Header.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on going through today's course! Hopefully, you've learned some valuable skills along the way and had fun doing it. Now it's time to put those skills to the test. In this assessment, you will train a new model that is able to recognize fresh and rotten fruit. You will need to get the model to a validation accuracy of `92%` in order to pass the assessment, though we challenge you to do even better if you can. You will have the use the skills that you learned in the previous exercises. Specifically, we suggest using some combination of transfer learning, data augmentation, and fine tuning. Once you have trained the model to be at least 92% accurate on the validation dataset, save your model, and then assess its accuracy. Let's get started! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will train a model to recognize fresh and rotten fruits. The dataset comes from [Kaggle](https://www.kaggle.com/sriramr/fruits-fresh-and-rotten-for-classification), a great place to go if you're interested in starting a project after this class. The dataset structure is in the `data/fruits` folder. There are 6 categories of fruits: fresh apples, fresh oranges, fresh bananas, rotten apples, rotten oranges, and rotten bananas. This will mean that your model will require an output layer of 6 neurons to do the categorization successfully. You'll also need to compile the model with `categorical_crossentropy`, as we have more than two categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fruits.png\" style=\"width: 600px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ImageNet Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encourage you to start with a model pretrained on ImageNet. Load the model with the correct weights, set an input shape, and choose to remove the last layers of the model. Remember that images have three dimensions: a height, and width, and a number of channels. Because these pictures are in color, there will be three channels for red, green, and blue. We've filled in the input shape for you. This cannot be changed or the assessment will fail. If you need a reference for setting up the pretrained model, please take a look at [notebook 05b](05b_presidential_doggy_door.ipynb) where we implemented transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "base_model = keras.applications.VGG16(\n",
    "    weights='imagenet',\n",
    "    input_shape=(224, 224, 3),\n",
    "    include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freeze Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we suggest freezing the base model, as done in [notebook 05b](05b_presidential_doggy_door.ipynb). This is done so that all the learning from the ImageNet dataset does not get destroyed in the initial training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze base model\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Layers to Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to add layers to the pretrained model. [Notebook 05b](05b_presidential_doggy_door.ipynb) can be used as a guide. Pay close attention to the last dense layer and make sure it has the correct number of neurons to classify the different types of fruit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inputs with correct shape\n",
    "inputs = keras.Input(shape=(224, 224, 3))\n",
    "\n",
    "x = base_model(inputs, training=False)\n",
    "\n",
    "# Add pooling layer or flatten layer\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Add final dense layer\n",
    "outputs = keras.layers.Dense(6, activation = 'softmax')(x)\n",
    "\n",
    "# Combine inputs and outputs to create model\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "vgg16 (Model)                (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 14,717,766\n",
      "Trainable params: 3,078\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to compile the model with loss and metrics options. Remember that we're training on a number of different categories, rather than a binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = keras.losses.CategoricalCrossentropy(from_logits=True) , metrics = [keras.metrics.CategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like, try to augment the data to improve the dataset. Feel free to look at [notebook 04a](04a_asl_augmentation.ipynb) and [notebook 05b](05b_presidential_doggy_door.ipynb) for augmentation examples. There is also documentation for the [Keras ImageDataGenerator class](https://keras.io/api/preprocessing/image/#imagedatagenerator-class). This step is optional, but it may be helpful to get to 92% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        samplewise_center=True,  # set each sample mean to 0\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to load the train and validation datasets. Pick the right folders, as well as the right `target_size` of the images (it needs to match the height and width input of the model you've created). If you'd like a reference, you can check out [notebook 05b](05b_presidential_doggy_door.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1182 images belonging to 6 classes.\n",
      "Found 329 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "# load and iterate training dataset\n",
    "train_it = datagen.flow_from_directory('data/fruits/train/', \n",
    "                                       target_size=(224, 224), \n",
    "                                       color_mode='rgb', \n",
    "                                       class_mode=\"categorical\")\n",
    "# load and iterate validation dataset\n",
    "valid_it = datagen.flow_from_directory('data/fruits/valid/', \n",
    "                                      target_size=(224, 224), \n",
    "                                      color_mode='rgb', \n",
    "                                      class_mode=\"categorical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to train the model! Pass the `train` and `valid` iterators into the `fit` function, as well as setting your desired number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "37/36 [==============================] - 19s 526ms/step - loss: 1.5577 - categorical_accuracy: 0.4788 - val_loss: 1.3894 - val_categorical_accuracy: 0.6596\n",
      "Epoch 2/30\n",
      "37/36 [==============================] - 19s 526ms/step - loss: 1.3634 - categorical_accuracy: 0.6827 - val_loss: 1.3122 - val_categorical_accuracy: 0.7356\n",
      "Epoch 3/30\n",
      "37/36 [==============================] - 19s 526ms/step - loss: 1.2808 - categorical_accuracy: 0.7640 - val_loss: 1.2404 - val_categorical_accuracy: 0.8024\n",
      "Epoch 4/30\n",
      "37/36 [==============================] - 19s 526ms/step - loss: 1.2490 - categorical_accuracy: 0.8029 - val_loss: 1.2227 - val_categorical_accuracy: 0.8237\n",
      "Epoch 5/30\n",
      "37/36 [==============================] - 19s 524ms/step - loss: 1.2262 - categorical_accuracy: 0.8249 - val_loss: 1.1996 - val_categorical_accuracy: 0.8480\n",
      "Epoch 6/30\n",
      "37/36 [==============================] - 20s 530ms/step - loss: 1.2149 - categorical_accuracy: 0.8274 - val_loss: 1.2081 - val_categorical_accuracy: 0.8389\n",
      "Epoch 7/30\n",
      "37/36 [==============================] - 19s 523ms/step - loss: 1.1630 - categorical_accuracy: 0.8900 - val_loss: 1.1298 - val_categorical_accuracy: 0.9210\n",
      "Epoch 8/30\n",
      "37/36 [==============================] - 19s 527ms/step - loss: 1.1177 - categorical_accuracy: 0.9340 - val_loss: 1.1422 - val_categorical_accuracy: 0.9088\n",
      "Epoch 9/30\n",
      "37/36 [==============================] - 19s 526ms/step - loss: 1.0944 - categorical_accuracy: 0.9560 - val_loss: 1.0996 - val_categorical_accuracy: 0.9605\n",
      "Epoch 10/30\n",
      "37/36 [==============================] - 19s 525ms/step - loss: 1.0835 - categorical_accuracy: 0.9687 - val_loss: 1.1044 - val_categorical_accuracy: 0.9453\n",
      "Epoch 11/30\n",
      "37/36 [==============================] - 19s 521ms/step - loss: 1.0770 - categorical_accuracy: 0.9738 - val_loss: 1.0975 - val_categorical_accuracy: 0.9544\n",
      "Epoch 12/30\n",
      "37/36 [==============================] - 19s 525ms/step - loss: 1.0761 - categorical_accuracy: 0.9738 - val_loss: 1.0866 - val_categorical_accuracy: 0.9574\n",
      "Epoch 13/30\n",
      "37/36 [==============================] - 19s 526ms/step - loss: 1.0726 - categorical_accuracy: 0.9763 - val_loss: 1.0875 - val_categorical_accuracy: 0.9574\n",
      "Epoch 14/30\n",
      "37/36 [==============================] - 19s 525ms/step - loss: 1.0630 - categorical_accuracy: 0.9856 - val_loss: 1.0897 - val_categorical_accuracy: 0.9635\n",
      "Epoch 15/30\n",
      "37/36 [==============================] - 20s 549ms/step - loss: 1.0618 - categorical_accuracy: 0.9848 - val_loss: 1.0873 - val_categorical_accuracy: 0.9666\n",
      "Epoch 16/30\n",
      "37/36 [==============================] - 19s 527ms/step - loss: 1.0642 - categorical_accuracy: 0.9831 - val_loss: 1.0847 - val_categorical_accuracy: 0.9696\n",
      "Epoch 17/30\n",
      "37/36 [==============================] - 19s 522ms/step - loss: 1.0615 - categorical_accuracy: 0.9873 - val_loss: 1.0810 - val_categorical_accuracy: 0.9666\n",
      "Epoch 18/30\n",
      "37/36 [==============================] - 20s 527ms/step - loss: 1.0595 - categorical_accuracy: 0.9890 - val_loss: 1.0776 - val_categorical_accuracy: 0.9635\n",
      "Epoch 19/30\n",
      "37/36 [==============================] - 19s 523ms/step - loss: 1.0582 - categorical_accuracy: 0.9882 - val_loss: 1.0789 - val_categorical_accuracy: 0.9635\n",
      "Epoch 20/30\n",
      "37/36 [==============================] - 19s 524ms/step - loss: 1.0569 - categorical_accuracy: 0.9915 - val_loss: 1.0780 - val_categorical_accuracy: 0.9666\n",
      "Epoch 21/30\n",
      "37/36 [==============================] - 19s 526ms/step - loss: 1.0589 - categorical_accuracy: 0.9865 - val_loss: 1.0788 - val_categorical_accuracy: 0.9666\n",
      "Epoch 22/30\n",
      "37/36 [==============================] - 19s 519ms/step - loss: 1.0572 - categorical_accuracy: 0.9907 - val_loss: 1.0805 - val_categorical_accuracy: 0.9666\n",
      "Epoch 23/30\n",
      "37/36 [==============================] - 19s 527ms/step - loss: 1.0550 - categorical_accuracy: 0.9924 - val_loss: 1.0741 - val_categorical_accuracy: 0.9726\n",
      "Epoch 24/30\n",
      "37/36 [==============================] - 20s 528ms/step - loss: 1.0559 - categorical_accuracy: 0.9915 - val_loss: 1.0777 - val_categorical_accuracy: 0.9666\n",
      "Epoch 25/30\n",
      "37/36 [==============================] - 20s 528ms/step - loss: 1.0546 - categorical_accuracy: 0.9915 - val_loss: 1.0773 - val_categorical_accuracy: 0.9666\n",
      "Epoch 26/30\n",
      "37/36 [==============================] - 19s 525ms/step - loss: 1.0543 - categorical_accuracy: 0.9915 - val_loss: 1.0710 - val_categorical_accuracy: 0.9757\n",
      "Epoch 27/30\n",
      "37/36 [==============================] - 19s 523ms/step - loss: 1.0537 - categorical_accuracy: 0.9932 - val_loss: 1.0726 - val_categorical_accuracy: 0.9757\n",
      "Epoch 28/30\n",
      "37/36 [==============================] - 19s 522ms/step - loss: 1.0563 - categorical_accuracy: 0.9898 - val_loss: 1.0669 - val_categorical_accuracy: 0.9787\n",
      "Epoch 29/30\n",
      "37/36 [==============================] - 19s 524ms/step - loss: 1.0529 - categorical_accuracy: 0.9924 - val_loss: 1.0676 - val_categorical_accuracy: 0.9757\n",
      "Epoch 30/30\n",
      "37/36 [==============================] - 19s 526ms/step - loss: 1.0524 - categorical_accuracy: 0.9924 - val_loss: 1.0782 - val_categorical_accuracy: 0.9666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4ac7cc9d30>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_it,\n",
    "          validation_data=valid_it,\n",
    "          steps_per_epoch=train_it.samples/train_it.batch_size,\n",
    "          validation_steps=valid_it.samples/valid_it.batch_size,\n",
    "          epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/36 [==============================] - 19s 524ms/step - loss: 1.0558 - categorical_accuracy: 0.9882 - val_loss: 1.0708 - val_categorical_accuracy: 0.9757\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4ac76caf28>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_it,\n",
    "          validation_data=valid_it,\n",
    "          steps_per_epoch=train_it.samples/train_it.batch_size,\n",
    "          validation_steps=valid_it.samples/valid_it.batch_size,\n",
    "          epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "37/36 [==============================] - 21s 560ms/step - loss: 1.5569 - categorical_accuracy: 0.7369 - val_loss: 0.7459 - val_categorical_accuracy: 0.8328\n",
      "Epoch 2/30\n",
      "37/36 [==============================] - 21s 573ms/step - loss: 0.6176 - categorical_accuracy: 0.8071 - val_loss: 0.8731 - val_categorical_accuracy: 0.7204\n",
      "Epoch 3/30\n",
      "37/36 [==============================] - 21s 566ms/step - loss: 0.5733 - categorical_accuracy: 0.8113 - val_loss: 1.0928 - val_categorical_accuracy: 0.6869\n",
      "Epoch 4/30\n",
      "37/36 [==============================] - 21s 567ms/step - loss: 1.8469 - categorical_accuracy: 0.7614 - val_loss: 0.6705 - val_categorical_accuracy: 0.8207\n",
      "Epoch 5/30\n",
      "37/36 [==============================] - 21s 576ms/step - loss: 0.7040 - categorical_accuracy: 0.8063 - val_loss: 0.6073 - val_categorical_accuracy: 0.8085\n",
      "Epoch 6/30\n",
      "37/36 [==============================] - 21s 556ms/step - loss: 0.6290 - categorical_accuracy: 0.8003 - val_loss: 0.6699 - val_categorical_accuracy: 0.7568\n",
      "Epoch 7/30\n",
      "37/36 [==============================] - 21s 569ms/step - loss: 0.9048 - categorical_accuracy: 0.7868 - val_loss: 0.7392 - val_categorical_accuracy: 0.7447\n",
      "Epoch 8/30\n",
      "37/36 [==============================] - 21s 572ms/step - loss: 0.7118 - categorical_accuracy: 0.8096 - val_loss: 0.8278 - val_categorical_accuracy: 0.8085\n",
      "Epoch 9/30\n",
      "37/36 [==============================] - 21s 559ms/step - loss: 0.7385 - categorical_accuracy: 0.8080 - val_loss: 0.6296 - val_categorical_accuracy: 0.7872\n",
      "Epoch 10/30\n",
      "37/36 [==============================] - 21s 578ms/step - loss: 0.7888 - categorical_accuracy: 0.7851 - val_loss: 0.7394 - val_categorical_accuracy: 0.7447\n",
      "Epoch 11/30\n",
      "37/36 [==============================] - 21s 571ms/step - loss: 1.0656 - categorical_accuracy: 0.8299 - val_loss: 0.6780 - val_categorical_accuracy: 0.8541\n",
      "Epoch 12/30\n",
      "37/36 [==============================] - 21s 570ms/step - loss: 0.6969 - categorical_accuracy: 0.7885 - val_loss: 1.4256 - val_categorical_accuracy: 0.7112\n",
      "Epoch 13/30\n",
      "37/36 [==============================] - 21s 575ms/step - loss: 0.6044 - categorical_accuracy: 0.8240 - val_loss: 0.9641 - val_categorical_accuracy: 0.6900\n",
      "Epoch 14/30\n",
      "37/36 [==============================] - 21s 567ms/step - loss: 0.5235 - categorical_accuracy: 0.8316 - val_loss: 2.4315 - val_categorical_accuracy: 0.8602\n",
      "Epoch 15/30\n",
      "37/36 [==============================] - 21s 565ms/step - loss: 1.4146 - categorical_accuracy: 0.7792 - val_loss: 0.5372 - val_categorical_accuracy: 0.8389\n",
      "Epoch 16/30\n",
      "37/36 [==============================] - 21s 571ms/step - loss: 0.7011 - categorical_accuracy: 0.8190 - val_loss: 0.9449 - val_categorical_accuracy: 0.8207\n",
      "Epoch 17/30\n",
      "37/36 [==============================] - 21s 559ms/step - loss: 0.7588 - categorical_accuracy: 0.7919 - val_loss: 0.7443 - val_categorical_accuracy: 0.8085\n",
      "Epoch 18/30\n",
      "37/36 [==============================] - 22s 582ms/step - loss: 0.5974 - categorical_accuracy: 0.8063 - val_loss: 0.3252 - val_categorical_accuracy: 0.8815\n",
      "Epoch 19/30\n",
      "37/36 [==============================] - 21s 571ms/step - loss: 0.8275 - categorical_accuracy: 0.8063 - val_loss: 1.3235 - val_categorical_accuracy: 0.7477\n",
      "Epoch 20/30\n",
      "37/36 [==============================] - 21s 556ms/step - loss: 1.1266 - categorical_accuracy: 0.7936 - val_loss: 0.7242 - val_categorical_accuracy: 0.8237\n",
      "Epoch 21/30\n",
      "37/36 [==============================] - 21s 559ms/step - loss: 0.5908 - categorical_accuracy: 0.8029 - val_loss: 0.5279 - val_categorical_accuracy: 0.8055\n",
      "Epoch 22/30\n",
      "37/36 [==============================] - 21s 572ms/step - loss: 0.5987 - categorical_accuracy: 0.8283 - val_loss: 0.7917 - val_categorical_accuracy: 0.7872\n",
      "Epoch 23/30\n",
      "37/36 [==============================] - 21s 567ms/step - loss: 0.7650 - categorical_accuracy: 0.8012 - val_loss: 0.5078 - val_categorical_accuracy: 0.8389\n",
      "Epoch 24/30\n",
      "37/36 [==============================] - 21s 565ms/step - loss: 0.8709 - categorical_accuracy: 0.7631 - val_loss: 0.4801 - val_categorical_accuracy: 0.8419\n",
      "Epoch 25/30\n",
      "37/36 [==============================] - 21s 566ms/step - loss: 0.4550 - categorical_accuracy: 0.8435 - val_loss: 0.6807 - val_categorical_accuracy: 0.7508\n",
      "Epoch 26/30\n",
      "37/36 [==============================] - 21s 571ms/step - loss: 0.7889 - categorical_accuracy: 0.8080 - val_loss: 0.5715 - val_categorical_accuracy: 0.8328\n",
      "Epoch 27/30\n",
      "37/36 [==============================] - 21s 572ms/step - loss: 0.5776 - categorical_accuracy: 0.8299 - val_loss: 0.7630 - val_categorical_accuracy: 0.8845\n",
      "Epoch 28/30\n",
      "37/36 [==============================] - 20s 552ms/step - loss: 0.8069 - categorical_accuracy: 0.8139 - val_loss: 0.4929 - val_categorical_accuracy: 0.8784\n",
      "Epoch 29/30\n",
      "37/36 [==============================] - 21s 571ms/step - loss: 1.0245 - categorical_accuracy: 0.8443 - val_loss: 0.5309 - val_categorical_accuracy: 0.8328\n",
      "Epoch 30/30\n",
      "37/36 [==============================] - 21s 562ms/step - loss: 0.4586 - categorical_accuracy: 0.8553 - val_loss: 0.4960 - val_categorical_accuracy: 0.8511\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4e74169e10>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_it,\n",
    "          validation_data=valid_it,\n",
    "          steps_per_epoch=train_it.samples/train_it.batch_size,\n",
    "          validation_steps=valid_it.samples/valid_it.batch_size,\n",
    "          epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "37/36 [==============================] - 21s 558ms/step - loss: 1.5320 - categorical_accuracy: 0.8012 - val_loss: 0.7496 - val_categorical_accuracy: 0.8207\n",
      "Epoch 2/30\n",
      "37/36 [==============================] - 21s 565ms/step - loss: 0.4935 - categorical_accuracy: 0.8469 - val_loss: 1.6327 - val_categorical_accuracy: 0.7508\n",
      "Epoch 3/30\n",
      "37/36 [==============================] - 21s 566ms/step - loss: 0.6721 - categorical_accuracy: 0.8071 - val_loss: 0.5774 - val_categorical_accuracy: 0.8328\n",
      "Epoch 4/30\n",
      "37/36 [==============================] - 21s 570ms/step - loss: 1.2083 - categorical_accuracy: 0.7115 - val_loss: 0.5897 - val_categorical_accuracy: 0.7903\n",
      "Epoch 5/30\n",
      "37/36 [==============================] - 21s 571ms/step - loss: 0.7926 - categorical_accuracy: 0.7665 - val_loss: 1.0763 - val_categorical_accuracy: 0.7751\n",
      "Epoch 6/30\n",
      "37/36 [==============================] - 21s 565ms/step - loss: 0.5174 - categorical_accuracy: 0.8376 - val_loss: 0.9082 - val_categorical_accuracy: 0.8298\n",
      "Epoch 7/30\n",
      "37/36 [==============================] - 21s 566ms/step - loss: 3.2103 - categorical_accuracy: 0.7572 - val_loss: 0.7586 - val_categorical_accuracy: 0.8450\n",
      "Epoch 8/30\n",
      "37/36 [==============================] - 21s 563ms/step - loss: 0.4883 - categorical_accuracy: 0.8536 - val_loss: 1.6346 - val_categorical_accuracy: 0.7508\n",
      "Epoch 9/30\n",
      "37/36 [==============================] - 21s 568ms/step - loss: 0.8750 - categorical_accuracy: 0.8325 - val_loss: 0.4944 - val_categorical_accuracy: 0.8754\n",
      "Epoch 10/30\n",
      "37/36 [==============================] - 21s 568ms/step - loss: 3.5038 - categorical_accuracy: 0.8232 - val_loss: 1.3899 - val_categorical_accuracy: 0.7994\n",
      "Epoch 11/30\n",
      "37/36 [==============================] - 21s 566ms/step - loss: 0.4907 - categorical_accuracy: 0.8621 - val_loss: 0.6037 - val_categorical_accuracy: 0.8450\n",
      "Epoch 12/30\n",
      "37/36 [==============================] - 21s 577ms/step - loss: 0.5763 - categorical_accuracy: 0.8553 - val_loss: 0.4603 - val_categorical_accuracy: 0.8754\n",
      "Epoch 13/30\n",
      "37/36 [==============================] - 21s 576ms/step - loss: 0.5371 - categorical_accuracy: 0.8435 - val_loss: 0.7886 - val_categorical_accuracy: 0.7538\n",
      "Epoch 14/30\n",
      "37/36 [==============================] - 21s 577ms/step - loss: 0.7088 - categorical_accuracy: 0.8249 - val_loss: 0.7293 - val_categorical_accuracy: 0.8176\n",
      "Epoch 15/30\n",
      "37/36 [==============================] - 21s 568ms/step - loss: 15.9082 - categorical_accuracy: 0.7970 - val_loss: 0.5008 - val_categorical_accuracy: 0.8419\n",
      "Epoch 16/30\n",
      "37/36 [==============================] - 21s 563ms/step - loss: 0.5423 - categorical_accuracy: 0.8359 - val_loss: 1.8300 - val_categorical_accuracy: 0.7052\n",
      "Epoch 17/30\n",
      "37/36 [==============================] - 21s 562ms/step - loss: 0.5581 - categorical_accuracy: 0.8418 - val_loss: 0.4897 - val_categorical_accuracy: 0.8723\n",
      "Epoch 18/30\n",
      "37/36 [==============================] - 21s 577ms/step - loss: 0.7370 - categorical_accuracy: 0.8181 - val_loss: 0.6446 - val_categorical_accuracy: 0.7872\n",
      "Epoch 19/30\n",
      "37/36 [==============================] - 21s 574ms/step - loss: 0.7422 - categorical_accuracy: 0.8249 - val_loss: 1.1320 - val_categorical_accuracy: 0.7720\n",
      "Epoch 20/30\n",
      "37/36 [==============================] - 21s 557ms/step - loss: 0.5693 - categorical_accuracy: 0.8393 - val_loss: 0.6117 - val_categorical_accuracy: 0.8541\n",
      "Epoch 21/30\n",
      "37/36 [==============================] - 21s 572ms/step - loss: 0.9798 - categorical_accuracy: 0.7910 - val_loss: 1.2554 - val_categorical_accuracy: 0.7751\n",
      "Epoch 22/30\n",
      "37/36 [==============================] - 21s 570ms/step - loss: 1.1581 - categorical_accuracy: 0.7817 - val_loss: 0.6191 - val_categorical_accuracy: 0.8085\n",
      "Epoch 23/30\n",
      "37/36 [==============================] - 21s 572ms/step - loss: 0.4109 - categorical_accuracy: 0.8570 - val_loss: 0.6666 - val_categorical_accuracy: 0.8419\n",
      "Epoch 24/30\n",
      "37/36 [==============================] - 21s 568ms/step - loss: 0.7792 - categorical_accuracy: 0.8105 - val_loss: 1.1161 - val_categorical_accuracy: 0.7386\n",
      "Epoch 25/30\n",
      "37/36 [==============================] - 21s 560ms/step - loss: 0.8146 - categorical_accuracy: 0.7944 - val_loss: 0.4581 - val_categorical_accuracy: 0.8298\n",
      "Epoch 26/30\n",
      "37/36 [==============================] - 21s 568ms/step - loss: 0.7333 - categorical_accuracy: 0.8367 - val_loss: 0.7833 - val_categorical_accuracy: 0.8085\n",
      "Epoch 27/30\n",
      "37/36 [==============================] - 21s 572ms/step - loss: 0.4671 - categorical_accuracy: 0.8587 - val_loss: 0.3652 - val_categorical_accuracy: 0.8784\n",
      "Epoch 28/30\n",
      "37/36 [==============================] - 21s 562ms/step - loss: 0.7168 - categorical_accuracy: 0.8147 - val_loss: 0.6804 - val_categorical_accuracy: 0.8541\n",
      "Epoch 29/30\n",
      "37/36 [==============================] - 21s 571ms/step - loss: 1.3359 - categorical_accuracy: 0.7893 - val_loss: 0.4764 - val_categorical_accuracy: 0.8511\n",
      "Epoch 30/30\n",
      "37/36 [==============================] - 21s 563ms/step - loss: 0.4925 - categorical_accuracy: 0.8359 - val_loss: 0.5731 - val_categorical_accuracy: 0.8845\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4e75b9dd68>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_it,\n",
    "          validation_data=valid_it,\n",
    "          steps_per_epoch=train_it.samples/train_it.batch_size,\n",
    "          validation_steps=valid_it.samples/valid_it.batch_size,\n",
    "          epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unfreeze Model for Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have reached 92% validation accuracy already, this next step is optional. If not, we suggest fine tuning the model with a very low learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the base model\n",
    "base_model.trainable = True\n",
    "\n",
    "# Compile the model with a low learning rate\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate = .00001),\n",
    "              loss = keras.losses.CategoricalCrossentropy(from_logits=True) , metrics = [keras.metrics.CategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_it,\n",
    "          validation_data=valid_it,\n",
    "          steps_per_epoch=train_it.samples/train_it.batch_size,\n",
    "          validation_steps=valid_it.samples/valid_it.batch_size,\n",
    "          epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, you now have a model that has a validation accuracy of 92% or higher. If not, you may want to go back and either run more epochs of training, or adjust your data augmentation. \n",
    "\n",
    "Once you are satisfied with the validation accuracy, evaluate the model by executing the following cell. The evaluate function will return a tuple, where the first value is your loss, and the second value is your accuracy. To pass, the model will need have an accuracy value of `92% or higher`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/10 [================================] - 4s 339ms/step - loss: 1.0660 - categorical_accuracy: 0.9848\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0660316944122314, 0.9848024249076843]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(valid_it, steps=valid_it.samples/valid_it.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess your model run the following two cells.\n",
    "\n",
    "**NOTE:** `run_assessment` assumes your model is named `model` and your validation data iterator is called `valid_it`. If for any reason you have modified these variable names, please update the names of the arguments passed to `run_assessment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_assessment import run_assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model 5 times to obtain average accuracy...\n",
      "\n",
      "11/10 [================================] - 4s 348ms/step - loss: 1.0664 - categorical_accuracy: 0.9757\n",
      "11/10 [================================] - 4s 346ms/step - loss: 1.0638 - categorical_accuracy: 0.9848\n",
      "11/10 [================================] - 4s 342ms/step - loss: 1.0746 - categorical_accuracy: 0.9696\n",
      "11/10 [================================] - 4s 342ms/step - loss: 1.0786 - categorical_accuracy: 0.9605\n",
      "11/10 [================================] - 4s 352ms/step - loss: 1.0801 - categorical_accuracy: 0.9635\n",
      "\n",
      "Accuracy required to pass the assessment is 0.92 or greater.\n",
      "Your average accuracy is 0.9708.\n",
      "\n",
      "Congratulations! You passed the assessment!\n",
      "See instructions below to generate a certificate.\n"
     ]
    }
   ],
   "source": [
    "run_assessment(model, valid_it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a Certificate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you passed the assessment, please return to the course page (shown below) and click the \"ASSESS TASK\" button, which will generate your certificate for the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/assess_task.png\" style=\"width: 800px;\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
